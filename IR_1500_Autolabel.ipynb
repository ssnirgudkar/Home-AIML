{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-1500-Autolabel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8b+DajG+sPOGODEW/+1/K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssnirgudkar/Home-AIML/blob/main/IR_1500_Autolabel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2BqcSJ3EFxr"
      },
      "source": [
        "#This code will be run for auto labeling the 1500 IR images on segments.ai. \n",
        "#It will use detectron, we will train the detectron using the Pilot IR, and then each of the batches within the 1500 images. \n",
        "#This way each time it will learn from the segmentation done on each batch and improve it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5-y0oWlvgdH"
      },
      "source": [
        "pip install --upgrade segments-ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e7NOkOfxDZW"
      },
      "source": [
        "#Establishing the api key for my segments.ai account \n",
        "from segments import SegmentsClient\n",
        "api_key = \"a89182567b17766b91773021b18d04574cd75109\"\n",
        "client = SegmentsClient(api_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_gzNefkxFty"
      },
      "source": [
        "# We will start with pilot IR 100 images that were handmade segmented as the start. Later on, we will add release file for each batch\n",
        "dataset_identifier = \"ssnirgudkar/PilotIR\"\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFYoMO_z3Aln"
      },
      "source": [
        "#install dependencies for detectron 2\n",
        "!pip install pyyaml==5.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__,torch.cuda.is_available())\n",
        "!gcc --version\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSjROww3Jmul"
      },
      "source": [
        "!pip install torch==1.8.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBrRV1hC5RNB"
      },
      "source": [
        "#install detectron2 (Colab has CUDA 10.1 + torch 1.8)\n",
        "import torch \n",
        "assert torch.__version__.startswith(\"1.8\") #need to manually install 1.8 only if colab changes it's default version. as we see above, the default is 1.8 rt now\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi3g9Xfh6e6f"
      },
      "source": [
        "#setup logger for detectron2 \n",
        "import detectron2 \n",
        "from detectron2.utils.logger import setup_logger \n",
        "setup_logger()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDvOAhHoxuus"
      },
      "source": [
        "\n",
        "#clone segments.ai repo for their utils code \n",
        "!git clone https://github.com/segments-ai/fast-labeling-workflow/\n",
        "%cd fast-labeling-workflow\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_86CvUp9VT4"
      },
      "source": [
        "# Initialize a SegmentsDataset from the release file for handmade manually segmented 100 IR images from pilot IR. \n",
        "#Later on release file for each batch will be added to this. \n",
        "from segments import SegmentsDataset\n",
        "from segments.utils import export_dataset\n",
        "\n",
        "release_file = client.get_release(dataset_identifier,'V2.0')\n",
        "#print(release_file)\n",
        "releasedataset = SegmentsDataset(release_file, labelset='segmentation', filter_by='labeled')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2MMmQ05CVrx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb53f6a8-2381-4e36-ae7f-f4c4fcc3fc71"
      },
      "source": [
        "# Now train the detectron model using the 100 image segmentation dataset(or additional batch release dataset)\n",
        "from utils import train_model\n",
        "model = train_model(releasedataset)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [00:02<00:00, 45.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Exported to ./export_ssnirgudkar_PilotIR_V2.0.json. Images and labels in segments/ssnirgudkar_PilotIR/V2.0\n",
            "\u001b[32m[06/03 22:53:38 d2.data.datasets.coco]: \u001b[0mLoaded 103 images in COCO format from ./export_ssnirgudkar_PilotIR_V2.0.json\n",
            "Metadata(evaluator_type='coco', image_root='segments/ssnirgudkar_PilotIR/V2.0', json_file='./export_ssnirgudkar_PilotIR_V2.0.json', name='my_dataset', thing_classes=['object', 'sky', 'water', 'warm entity', 'background', 'cold entity', 'sun'])\n",
            "\u001b[32m[06/03 22:53:49 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 7, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/03 22:53:49 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[06/03 22:53:49 d2.data.datasets.coco]: \u001b[0mLoaded 103 images in COCO format from ./export_ssnirgudkar_PilotIR_V2.0.json\n",
            "\u001b[32m[06/03 22:53:49 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 103 images left.\n",
            "\u001b[32m[06/03 22:53:49 d2.data.build]: \u001b[0mDistribution of instances among all 7 categories:\n",
            "\u001b[36m|  category   | #instances   |  category  | #instances   |  category   | #instances   |\n",
            "|:-----------:|:-------------|:----------:|:-------------|:-----------:|:-------------|\n",
            "|   object    | 0            |    sky     | 103          |    water    | 103          |\n",
            "| warm entity | 44           | background | 103          | cold entity | 103          |\n",
            "|     sun     | 19           |            |              |             |              |\n",
            "|    total    | 475          |            |              |             |              |\u001b[0m\n",
            "\u001b[32m[06/03 22:53:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[06/03 22:53:49 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[06/03 22:53:49 d2.data.common]: \u001b[0mSerializing 103 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[06/03 22:53:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.93 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[06/03 22:53:49 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "model_final_f10217.pkl: 178MB [00:02, 80.1MB/s]                           \n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (28, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (28,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (7, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/03 22:53:53 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/detectron2/data/detection_utils.py:419: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n",
            "/usr/local/lib/python3.7/dist-packages/detectron2/data/detection_utils.py:419: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
            "  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[06/03 22:54:01 d2.utils.events]: \u001b[0m eta: 0:01:49  iter: 19  total_loss: 3.392  loss_cls: 1.912  loss_box_reg: 0.6124  loss_mask: 0.6914  loss_rpn_cls: 0.2163  loss_rpn_loc: 0.1129  time: 0.3815  data_time: 0.0179  lr: 4.9953e-06  max_mem: 2231M\n",
            "\u001b[32m[06/03 22:54:09 d2.utils.events]: \u001b[0m eta: 0:01:41  iter: 39  total_loss: 3.547  loss_cls: 1.78  loss_box_reg: 0.5969  loss_mask: 0.6875  loss_rpn_cls: 0.2821  loss_rpn_loc: 0.1554  time: 0.3826  data_time: 0.0088  lr: 9.9902e-06  max_mem: 2280M\n",
            "\u001b[32m[06/03 22:54:17 d2.utils.events]: \u001b[0m eta: 0:01:33  iter: 59  total_loss: 3.151  loss_cls: 1.557  loss_box_reg: 0.5995  loss_mask: 0.6774  loss_rpn_cls: 0.2021  loss_rpn_loc: 0.1294  time: 0.3846  data_time: 0.0087  lr: 1.4985e-05  max_mem: 2280M\n",
            "\u001b[32m[06/03 22:54:24 d2.utils.events]: \u001b[0m eta: 0:01:25  iter: 79  total_loss: 2.99  loss_cls: 1.269  loss_box_reg: 0.6339  loss_mask: 0.6644  loss_rpn_cls: 0.1292  loss_rpn_loc: 0.1554  time: 0.3861  data_time: 0.0074  lr: 1.998e-05  max_mem: 2280M\n",
            "\u001b[32m[06/03 22:54:32 d2.utils.events]: \u001b[0m eta: 0:01:17  iter: 99  total_loss: 2.602  loss_cls: 0.9275  loss_box_reg: 0.6529  loss_mask: 0.6475  loss_rpn_cls: 0.07956  loss_rpn_loc: 0.1279  time: 0.3861  data_time: 0.0075  lr: 2.4975e-05  max_mem: 2280M\n",
            "\u001b[32m[06/03 22:54:40 d2.utils.events]: \u001b[0m eta: 0:01:10  iter: 119  total_loss: 2.312  loss_cls: 0.7436  loss_box_reg: 0.6953  loss_mask: 0.632  loss_rpn_cls: 0.06039  loss_rpn_loc: 0.09739  time: 0.3874  data_time: 0.0078  lr: 2.997e-05  max_mem: 2280M\n",
            "\u001b[32m[06/03 22:54:48 d2.utils.events]: \u001b[0m eta: 0:01:02  iter: 139  total_loss: 2.255  loss_cls: 0.6847  loss_box_reg: 0.7425  loss_mask: 0.6105  loss_rpn_cls: 0.049  loss_rpn_loc: 0.1291  time: 0.3889  data_time: 0.0074  lr: 3.4965e-05  max_mem: 2312M\n",
            "\u001b[32m[06/03 22:54:56 d2.utils.events]: \u001b[0m eta: 0:00:54  iter: 159  total_loss: 2.137  loss_cls: 0.6346  loss_box_reg: 0.7123  loss_mask: 0.5867  loss_rpn_cls: 0.06075  loss_rpn_loc: 0.1091  time: 0.3904  data_time: 0.0078  lr: 3.996e-05  max_mem: 2312M\n",
            "\u001b[32m[06/03 22:55:04 d2.utils.events]: \u001b[0m eta: 0:00:47  iter: 179  total_loss: 2.157  loss_cls: 0.619  loss_box_reg: 0.7601  loss_mask: 0.5548  loss_rpn_cls: 0.03554  loss_rpn_loc: 0.09956  time: 0.3929  data_time: 0.0085  lr: 4.4955e-05  max_mem: 2312M\n",
            "\u001b[32m[06/03 22:55:12 d2.utils.events]: \u001b[0m eta: 0:00:39  iter: 199  total_loss: 2.035  loss_cls: 0.5637  loss_box_reg: 0.7198  loss_mask: 0.5258  loss_rpn_cls: 0.04372  loss_rpn_loc: 0.1171  time: 0.3935  data_time: 0.0091  lr: 4.995e-05  max_mem: 2312M\n",
            "\u001b[32m[06/03 22:55:21 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 219  total_loss: 1.979  loss_cls: 0.5123  loss_box_reg: 0.7489  loss_mask: 0.5187  loss_rpn_cls: 0.04577  loss_rpn_loc: 0.09188  time: 0.3971  data_time: 0.0073  lr: 5.4945e-05  max_mem: 2312M\n",
            "\u001b[32m[06/03 22:55:30 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 239  total_loss: 1.903  loss_cls: 0.5136  loss_box_reg: 0.7266  loss_mask: 0.4884  loss_rpn_cls: 0.03192  loss_rpn_loc: 0.1228  time: 0.4001  data_time: 0.0080  lr: 5.994e-05  max_mem: 2316M\n",
            "\u001b[32m[06/03 22:55:38 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 259  total_loss: 1.855  loss_cls: 0.4922  loss_box_reg: 0.7319  loss_mask: 0.4749  loss_rpn_cls: 0.03299  loss_rpn_loc: 0.09434  time: 0.4018  data_time: 0.0082  lr: 6.4935e-05  max_mem: 2316M\n",
            "\u001b[32m[06/03 22:55:47 d2.utils.events]: \u001b[0m eta: 0:00:08  iter: 279  total_loss: 1.891  loss_cls: 0.4906  loss_box_reg: 0.7664  loss_mask: 0.4806  loss_rpn_cls: 0.02871  loss_rpn_loc: 0.1071  time: 0.4035  data_time: 0.0073  lr: 6.993e-05  max_mem: 2316M\n",
            "\u001b[32m[06/03 22:55:56 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 1.823  loss_cls: 0.4128  loss_box_reg: 0.7358  loss_mask: 0.4456  loss_rpn_cls: 0.02137  loss_rpn_loc: 0.0965  time: 0.4061  data_time: 0.0085  lr: 7.4925e-05  max_mem: 2316M\n",
            "\u001b[32m[06/03 22:55:57 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:02:01 (0.4061 s / it)\n",
            "\u001b[32m[06/03 22:55:57 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:02 (0:00:01 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT3bTuCyErKr"
      },
      "source": [
        "# This is the dataset that we have to infer using the model\n",
        "dataset_identifier_infer = \"ssnirgudkar/IR-2020-10-20-15-52-18\"\n",
        "dataset_infer = client.get_dataset(dataset_identifier_infer)\n",
        "\n",
        "#dataset_identifier_trial = \"ssnirgudkar/trial\"\n",
        "#dataset_infer_trial = client.get_dataset(dataset_identifier_trial)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHwnBangFVB0"
      },
      "source": [
        "# Get images that are not in labeled status for auto-labeling \n",
        "infer_release_file = client.get_release(dataset_identifier_infer,'V1.0')\n",
        "infer_releasedataset = SegmentsDataset(infer_release_file, labelset='ground-truth', filter_by=['Unlabeled','Skipped','Prelabeled'])\n",
        "\n",
        "\n",
        "#trial_release_file = client.get_release(dataset_identifier_trial,'V1.1')\n",
        "#trial_releasedataset = SegmentsDataset(trial_release_file, labelset='ground-truth', filter_by=['Unlabeled','Skipped','Prelabeled'])\n",
        "#trial_releasedataset = SegmentsDataset(trial_release_file, labelset='ground-truth', filter_by=['Prelabeled'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDwfYGC2Gpc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4839fd0e-8861-4882-9d7f-8ee5fa9d576e"
      },
      "source": [
        "from segments import SegmentsDataset\n",
        "from segments.utils import export_dataset\n",
        "from segments.utils import bitmap2file\n",
        "\n",
        "import cv2 \n",
        "import numpy as np \n",
        "import os \n",
        "from utils import visualize \n",
        "#this needs 3 channel image. so had to convert grayscale IR to RGB - 3 channels by copying the same pixel value on all 3\n",
        "#add logic to check if the image is 1 channel, and if so convert it to 3 channel\n",
        "for sample in infer_releasedataset:\n",
        "#for sample in trial_releasedataset:\n",
        "   infer_image = sample['image'] \n",
        "      \n",
        "   #imgcv = cv2.imread(infer_image)\n",
        "   #assert not isinstance(imgcv,type(None)), 'image not found'\n",
        "   #print (\"file read = {0}\".format(imgcv))\n",
        "   #print(os.getcwd())  # prints current working directory\n",
        "   #infer_image2 = RemoveAlphaChannel[infer_image, White]\n",
        "   #4channel CYMK image is not taken by detectron. it needs 3 channels. so converting to RGB. we can add a conditional check\n",
        "   # to first check if it's 4 channels and then convert only if it is\n",
        "   infer_image2 = infer_image.convert(\"RGB\")\n",
        "   segmentation_bitmap, annotations = model(infer_image2)\n",
        "   # This message gets printed for each image \n",
        "   print(\"segmentation bitmap executed\")\n",
        "    \n",
        "\n",
        "# Upload the predictions to Segments.ai so that we can check the effectiveness of auto labels\n",
        "   file = bitmap2file(segmentation_bitmap)\n",
        "   asset = client.upload_asset(file, 'label.png')    \n",
        "   attributes = {\n",
        "        'format_version': '0.1',\n",
        "        'annotations': annotations,\n",
        "        'segmentation_bitmap': { 'url': asset['url'] },\n",
        "   }\n",
        "   client.add_label(sample['uuid'], 'ground-truth', attributes, label_status='PRELABELED')\n",
        "   \n",
        "  "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n",
            "segmentation bitmap executed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}